{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1. Softmax Regression.\n",
    "\n",
    "В этом домашнем задании вам требуется написать свою модель Логистической Регрессии для решения задачи многоклассовой классификации. Для решения задания допускается использование библиотек `numpy` и `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "Задание состоит из набора функций и классов, которые надо верно заполнить:\n",
    "\n",
    "1) **(0.5 баллов)** Реализуйте функцию `softmax` для подсчета вероятностей. Проверьте свое решение локально с помощью команды: `python run.py unittest softmax`. Не забудьте учесть **переполнение типов в Numpy**.\n",
    "\n",
    "2) **(0.5 баллов)** Реализуйте функцию `one_hot_encode` для one-hot кодирования меток таргета. Проверьте свое решение локально с помощью команды: `python run.py unittest ohe`.\n",
    "\n",
    "3) **(3 балла)** Реализуйте класс `StandartScaler` для приведения признаков к стандартному нормальному распределению $\\sim \\mathcal{N}(0, 1)$. Проверьте свое решение локально с помощью команды: `python run.py unittest scaler`. Для предотвращения ошибки деления на 0 используйте значение параметра `eps=1e-45`.\n",
    "\n",
    "4) Реализуйте класс `SoftmaxRegression` для обучения многоклассовой логистической регрессии mini-batch градиентного спуска. Для этого реализуйте методы:\n",
    "    * **(0.5 баллов)** `predict_proba` - метод для предсказания вероятностей принадлежности объектов к каждому из классов. Проверьте свое решение локально с помощью команды: `python run.py unittest predict_proba`.\n",
    "    * **(0.5 баллов)** `predict` - метод для предсказания меток класса. Проверьте свое решение локально с помощью команды: `python run.py unittest predict`.\n",
    "    * **(0.5 баллов)** `loss` - метод для подсчета функции ошибки - CrossEntropy. Проверьте свое решение локально с помощью команды: `python run.py unittest loss`. Обратите внимание, что агрегация значения лосса по батчу производится усреднением.\n",
    "    * **(2 балла)** `grad` - метод для подсчета градиента функции ошибки для логистической регрессии. Проверьте свое решение локально с помощью команды: `python run.py unittest grad`.\n",
    "    * **(0.5 баллов)** `step` - метод шага обновления параметров модели **методом градиентного спуска**. Проверьте свое решение локально с помощью команды: `python run.py unittest step`.\n",
    "    * **(2 балла)** `fit` - метод для обучения весов модели методом mini-batch градиентного спуска. Проверьте свое решение локально с помощью команды: `python run.py unittest fit`.\n",
    "5) Реализуйте пайплан решения задачи классификации с помощью класса `Trainer`.\n",
    "    * метод `train_model` принимает данные, обрабатывает, генерирует новые признаки и обучает на них модель. \n",
    "    * метод `predict_model` принимает тестовые данные, делает аналогичные преобразования как в `train_model` и используя обученную модель делает предсказания меток класса. Реализация вашего пайплайна будет проверена с помощью заготовленных данных и подсчета метрики accuracy. Данные можно поисследовать в ноутбуке `EDA.ipynb`, загрузка этого ноутбука в репозиторий необязательна, а использование по желанию. Запустите проверку решения локально с помощью команды `python run.py grade classifier`. Проверка выдаст результат на валидационной выборке. Финальная метрика будет считаться в системе на другой отложенной выборке. Оценка за этот пункт ДЗ выставляется согласно следующим правилам:\n",
    "\n",
    "<div align=\"center\">\n",
    "   \n",
    "|  | Оценка |\n",
    "| --------: | :--------: |\n",
    "| 0.00 <= accuracy < 0.25 | 0 |\n",
    "| 0.25 <= accuracy < 0.30 | 3 |\n",
    "| 0.30 <= accuracy < 0.70 | 4 |\n",
    "| 0.70 <= accuracy < 0.78 | 5 |\n",
    "| 0.78 <= accuracy < 0.80 | 6 |\n",
    "| 0.80 <= accuracy < 0.85 | 7 |\n",
    "| 0.85 <= accuracy < 0.92| 8 | \n",
    "| 0.92 <= accuracy < 0.95| 9 |\n",
    "| 0.95 <= accuracy < 1.00| 10|\n",
    "\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Вычисляет softmax функцию для входного массива x.\n",
    "    \n",
    "    Softmax функция преобразует входные значения в вероятности, распределяя\n",
    "    их таким образом, что их сумма равна 1. Это полезно в задачах классификации,\n",
    "    где требуется получить вероятности принадлежности к каждому классу.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Входной массив значений размером (n_samples, n_classes), для которых необходимо вычислить softmax.\n",
    "    \n",
    "    Возвращает:\n",
    "    ----------\n",
    "    numpy.ndarray\n",
    "        Массив значений softmax, где каждый элемент является вероятностью, и сумма всех элементов равна 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "\n",
    "    exp_x = np.exp(x_shifted)\n",
    "\n",
    "    sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "def one_hot_encode(y, n_classes=None):\n",
    "    \"\"\"\n",
    "    Выполняет one-hot кодирование для заданного списка меток.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    y : numpy.ndarray или list\n",
    "        Вектор или список меток классов, которые необходимо закодировать.\n",
    "        Значения меток должны быть целыми числами от 0 до n_classes-1.\n",
    "\n",
    "    n_classes : int или None, по умолчанию None\n",
    "        Количество классов (размерность выходного пространства).\n",
    "        Если None, то количество классов определяется автоматически как максимум значения в y плюс один.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    numpy.ndarray\n",
    "        Массив размером (n_samples, n_classes), где n_samples — количество образцов, а n_classes — количество классов.\n",
    "        Каждая строка представляет собой one-hot закодированное представление соответствующей метки из y.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(y) + 1\n",
    "\n",
    "    one_hot = np.zeros((len(y), n_classes))\n",
    "\n",
    "    one_hot[np.arange(len(y)), y] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Класс для стандартизации данных путем удаления среднего и масштабирования к единичной дисперсии.\n",
    "\n",
    "    Стандартизация данных улучшает производительность большинства алгоритмов машинного обучения,\n",
    "    приводя их к единому масштабу. Класс `StandardScaler` вычисляет среднее и дисперсию по\n",
    "    обучающим данным и использует эти параметры для стандартизации новых данных.\n",
    "\n",
    "    Атрибуты:\n",
    "    ---------\n",
    "    mean_ : numpy.ndarray размера (n_features,) или None\n",
    "        Среднее значение каждого признака в обучающем наборе данных. Инициализируется как None до вызова метода `fit`.\n",
    "\n",
    "    var_ : numpy.ndarray размера (n_features,) или None\n",
    "        Дисперсия каждого признака в обучающем наборе данных. Инициализируется как None до вызова метода `fit`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализирует объект класса StandardScaler.\n",
    "\n",
    "        Инициализирует атрибуты mean_ и var_ как None, которые будут заполнены\n",
    "        после выполнения метода `fit`.\n",
    "        \"\"\"\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "        self.eps = 1e-45\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Вычисляет среднее и дисперсию для каждого признака в обучающем наборе данных.\n",
    "\n",
    "        Метод `fit` обучает модель на данных, вычисляя среднее и дисперсию, которые будут\n",
    "        использоваться для стандартизации данных.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив данных размером (n_samples, n_features), где n_samples — количество образцов,\n",
    "            а n_features — количество признаков.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : StandardScaler\n",
    "            Возвращает экземпляр объекта `StandardScaler` с вычисленными атрибутами mean_ и var_.\n",
    "        \"\"\"\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.var_ = np.var(X, axis=0)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Преобразует данные, применяя стандартизацию на основе среднего и дисперсии, вычисленных в методе `fit`.\n",
    "\n",
    "        Метод `transform` стандартизирует новые данные на основе параметров, вычисленных в `fit`.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив данных размером (n_samples, n_features), который необходимо стандартизировать.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        X_scaled : numpy.ndarray\n",
    "            Стандартизированные данные того же размера, что и входной массив X.\n",
    "        \"\"\"\n",
    "\n",
    "        return (X - self.mean_) / np.sqrt(self.var_ + self.eps)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Комбинированный метод для выполнения обучения и трансформации данных.\n",
    "\n",
    "        Этот метод сначала вычисляет среднее и дисперсию по обучающим данным,\n",
    "        а затем сразу же стандартизирует их.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив данных размером (n_samples, n_features), который необходимо обучить и стандартизировать.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        X_scaled : numpy.ndarray\n",
    "            Стандартизированные данные того же размера, что и входной массив X.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Класс для выполнения многоклассовой логистической регрессии с использованием softmax-функции\n",
    "    и поддержкой L1 и L2 регуляризации.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    lr : float, default=0.01\n",
    "        Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "    weight_decay : float, default=0\n",
    "        Коэффициент регуляризации, который предотвращает переобучение путем добавления штрафа\n",
    "        за большие значения весов. В зависимости от параметра `penalty`, может использоваться\n",
    "        для L1 или L2 регуляризации.\n",
    "\n",
    "    n_epochs : int, default=100\n",
    "        Количество эпох для обучения модели.\n",
    "\n",
    "    penalty : str, default='l2'\n",
    "        Тип регуляризации. Поддерживаются значения 'l1' для L1-регуляризации и 'l2' для L2-регуляризации.\n",
    "\n",
    "    batch_size : int, default=16\n",
    "        Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "    \n",
    "    Атрибуты:\n",
    "    ---------\n",
    "    coef_ : numpy.ndarray или None\n",
    "        Коэффициенты (веса) модели размером (n_features, n_classes), которые обучаются на данных. Инициализируются как None до вызова метода `fit`.\n",
    "\n",
    "    intercept_ : numpy.ndarray или None\n",
    "        Свободный член (сдвиг) модели размером (n_classes). Инициализируется как None до вызова метода `fit`.\n",
    "\n",
    "    self.n_classes_ : int\n",
    "        Количество классов, определяемое на основе уникальных меток в обучающем наборе данных.\n",
    "        Этот параметр устанавливается после вызова метода `fit` и используется для определения \n",
    "        размерности выходного пространства модели. Он равен максимальному значению метки в данных плюс один.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, weight_decay=0, n_epochs=100, penalty='l2', batch_size=16, fit_intercept=True):\n",
    "        \"\"\"\n",
    "        Инициализация объекта класса SoftmaxRegression с заданными гиперпараметрами.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        lr : float, default=0.01\n",
    "            Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "        weight_decay : float, default=0\n",
    "            Коэффициент регуляризации.\n",
    "\n",
    "        n_epochs : int, default=100\n",
    "            Количество эпох для обучения модели.\n",
    "\n",
    "        penalty : str, default='l2'\n",
    "            Тип регуляризации. 'l1' для L1-регуляризации, 'l2' для L2-регуляризации.\n",
    "\n",
    "        batch_size : int, default=16\n",
    "            Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "        fit_intercept : bool, по умолчанию True\n",
    "            Включать ли свободный член (сдвиг) в модель.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_epochs = n_epochs\n",
    "        self.penalty = penalty\n",
    "        self.batch_size = batch_size\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.n_classes_ = None\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает вероятности классов для входных данных на основе обученной модели softmax-регрессии.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features).\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        numpy.ndarray\n",
    "            Массив предсказанных вероятностей для каждого класса, где каждая строка соответствует одному объекту.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"Model has not been fitted yet.\")\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "            weights = np.vstack([self.intercept_, self.coef_])\n",
    "        else:\n",
    "            weights = self.coef_\n",
    "\n",
    "        logits = np.dot(X, weights)\n",
    "\n",
    "        return softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для входных данных на основе обученной модели softmax-регрессии.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features).\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        numpy.ndarray\n",
    "            Вектор предсказанных меток классов (значения от 0 до n_classes-1).\n",
    "        \"\"\"\n",
    "\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "\n",
    "    def loss(self, y_true, probs):\n",
    "        \"\"\"\n",
    "        Вычисляет функцию потерь для многоклассовой классификации на основе кросс-энтропии\n",
    "        с учетом L1/L2 регуляризации\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        y_true : numpy.ndarray\n",
    "            Вектор истинных меток классов (значения от 0 до n_classes-1).\n",
    "\n",
    "        probs : numpy.ndarray\n",
    "            Массив предсказанных вероятностей для каждого класса.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        float\n",
    "            Значение функции потерь на основе кросс-энтропии.\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples = y_true.shape[0]\n",
    "\n",
    "        y_one_hot = one_hot_encode(y_true, self.n_classes_)\n",
    "\n",
    "        log_probs = -np.log(probs + 1e-45)\n",
    "        cross_entropy_loss = np.sum(y_one_hot * log_probs) / n_samples\n",
    "\n",
    "        reg_term = 0\n",
    "        if self.penalty == 'l2':\n",
    "            reg_term = self.weight_decay * np.sum(self.coef_ ** 2)\n",
    "        elif self.penalty == 'l1':\n",
    "            reg_term = self.weight_decay * np.sum(np.abs(self.coef_))\n",
    "\n",
    "        return cross_entropy_loss + reg_term\n",
    "\n",
    "\n",
    "      \n",
    "    def loss_grad(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Вычисляет градиент функции потерь по отношению к весам модели для softmax-регрессии.\n",
    "\n",
    "        В случае использования регуляризации, градиент включает соответствующие компоненты для\n",
    "        штрафа за большие значения весов.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features), где n_samples — количество образцов,\n",
    "            а n_features — количество признаков.\n",
    "\n",
    "        y_true : numpy.ndarray\n",
    "            Вектор истинных меток классов (значения от 0 до n_classes-1).\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        grad : numpy.ndarray\n",
    "            Градиент функции потерь по отношению к весам модели.\n",
    "\n",
    "        grad_intercept : numpy.ndarray\n",
    "            Градиент функции потерь по отношению к свободному члену.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        probs = self.predict_proba(X)\n",
    "\n",
    "        y_one_hot = one_hot_encode(y_true, self.n_classes_)\n",
    "\n",
    "        error = probs - y_one_hot\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            grad_intercept = np.sum(error, axis=0) / n_samples\n",
    "        else:\n",
    "            grad_intercept = None\n",
    "\n",
    "        grad_coef = np.dot(X.T, error) / n_samples\n",
    "\n",
    "        if self.penalty == 'l2':\n",
    "            grad_coef += 2 * self.weight_decay * self.coef_\n",
    "        elif self.penalty == 'l1':\n",
    "            grad_coef += self.weight_decay * np.sign(self.coef_)\n",
    "\n",
    "        return grad_coef, grad_intercept\n",
    "\n",
    "\n",
    "    def step(self, grad, grad_intercept):\n",
    "        \"\"\"\n",
    "        Выполняет один шаг обновления весов модели с использованием вычисленного градиента.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        grad : numpy.ndarray\n",
    "            Градиент функции потерь по отношению к весам модели (размером как coef_).\n",
    "        \n",
    "        grad_intercept : numpy.ndarray или None\n",
    "            Градиент функции потерь по отношению к свободному члену (размером как intercept_).\n",
    "            Если fit_intercept=False, этот параметр будет равен None.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.coef_ -= self.lr * grad\n",
    "        if self.fit_intercept and grad_intercept is not None:\n",
    "            self.intercept_ -= self.lr * grad_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель softmax-регрессии с использованием mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features), где n_samples — количество образцов,\n",
    "            а n_features — количество признаков.\n",
    "\n",
    "        y : numpy.ndarray\n",
    "            Вектор истинных меток классов (значения от 0 до n_classes-1).\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : SoftmaxRegression\n",
    "            Обученная модель softmax-регрессии.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_classes_ = np.max(y) + 1\n",
    "\n",
    "        self.coef_ = np.zeros((n_features, self.n_classes_))\n",
    "        self.intercept_ = np.zeros(self.n_classes_) if self.fit_intercept else None\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "\n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "\n",
    "                grad_coef, grad_intercept = self.loss_grad(X_batch, y_batch)\n",
    "\n",
    "                self.step(grad_coef, grad_intercept)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Класс для управления процессом обучения и предсказания с использованием модели машинного обучения.\n",
    "\n",
    "    Этот класс предоставляет методы для обучения модели на данных и выполнения предсказаний на новых данных.\n",
    "    Он инкапсулирует логику обучения и предсказания, обеспечивая удобный интерфейс для выполнения этих задач.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.028, weight_decay=0.0001, n_epochs=400, penalty='l2', batch_size=15, fit_intercept=True):\n",
    "        \"\"\"\n",
    "        Инициализирует объект класса Trainer.\n",
    "\n",
    "        Этот конструктор может использоваться для инициализации необходимых атрибутов, таких как модель,\n",
    "        средства для предобработки данных, или другие параметры, необходимые для обучения и предсказания.\n",
    "        \"\"\"\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = SoftmaxRegression(lr=lr, weight_decay=weight_decay, n_epochs=n_epochs, \n",
    "        penalty=penalty, batch_size=batch_size, fit_intercept=fit_intercept)\n",
    "\n",
    "        \n",
    "    def train_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель машинного обучения на предоставленных данных.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features), где n_samples — количество образцов,\n",
    "            а n_features — количество признаков.\n",
    "\n",
    "        y : numpy.ndarray или pandas.Series\n",
    "            Вектор меток классов, соответствующих каждому объекту из X.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : Trainer\n",
    "            Возвращает экземпляр текущего объекта класса Trainer после обучения модели.\n",
    "        \"\"\"\n",
    "\n",
    "        X_train_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        self.model.fit(X_train_scaled, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict_model(self, X):\n",
    "        \"\"\"\n",
    "        Делает предсказания на новых данных с использованием обученной модели.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features) для новых данных.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        predictions : numpy.ndarray\n",
    "            Вектор предсказанных меток классов для входных данных.\n",
    "        \"\"\"\n",
    "\n",
    "        X_test_scaled = self.scaler.transform(X)\n",
    "\n",
    "        predictions = self.model.predict(X_test_scaled)\n",
    "\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
