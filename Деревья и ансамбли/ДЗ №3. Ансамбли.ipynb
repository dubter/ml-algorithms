{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d96058a-d06c-44c1-96e8-d50af48acec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ДЗ №3. Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ac7e3-bdd4-480c-a3c9-b18e3fae1840",
   "metadata": {},
   "source": [
    "В этом домашнем задании вам требуется написать методы ансамблирования для решения задачи многоклассовой классификации. Для решения задания допускается использование библиотек `numpy`, `pandas` и `scikit-learn`. Решение сдается коммитом в этот репозиторий заполненного файла `solution.py`. За задание максимально можно набрать **40 баллов**. Вес каждого пункта указан в самом задании.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fbf52-952a-467f-b1a1-603392b2ce84",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d607a-374d-424a-8ad9-66ca43d5b457",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209768ae-2c94-414b-8d49-882b0a132582",
   "metadata": {},
   "source": [
    "В задании требуется вписать код в местах `# CODE HERE` и `...`.\n",
    "\n",
    "#### Random Forest (10 баллов)\n",
    "   * Напишите ансамбль типа `RandomForest` для решения задачи классификации. \n",
    "   * Подберите оптимальные гиперпараметры модели и установите их в качестве значений по умолчанию. Проверьте свое решение с помощью теста.\n",
    "   \n",
    "> Подобранная конфигурация ансамбля должна проходить все тесты задания.\n",
    "\n",
    "```python run.py unittest random_forest```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faef8b7b-d0a5-47ec-8e70-9a0cbd114291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b57a9ab-b8b5-4539-b188-5cbf2aaef726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    \"\"\"\n",
    "    Модель случайного леса (RandomForest).\n",
    "\n",
    "    Атрибуты:\n",
    "    ----------\n",
    "    n_estimators : int\n",
    "        Количество деревьев в лесу.\n",
    "    \n",
    "    bootstrap : bool\n",
    "        Используется ли бутстрап при построении деревьев. \n",
    "        Если False, то для каждого дерева используется весь набор данных.\n",
    "    \n",
    "    estimators : list\n",
    "        Список деревьев с заданными параметрами (**kwargs).\n",
    "    \n",
    "    kwargs : dict\n",
    "        Параметры для каждого дерева, такие как min_samples_split, max_depth и др.\n",
    "        Передаются в DecisionTreeClassifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, bootstrap=True, max_depth=10, min_samples_split=2, min_samples_leaf=1, max_feature='sqrt', **kwargs):\n",
    "        self.bootstrap = bootstrap\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_feature = max_feature\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает случайный лес на тренировочной выборке (X, y). \n",
    "        Если bootstrap=False, используется весь набор данных для каждого дерева,\n",
    "        если bootstrap=True, используются бутстрап-выборки.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Тренировочные данные (признаки).\n",
    "        \n",
    "        y : np.array, shape (n_samples,)\n",
    "            Целевые метки классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : RandomForestClassifier\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                X_sample, y_sample = resample(X, y)\n",
    "            else:\n",
    "                X_sample, y_sample = X, y\n",
    "\n",
    "            tree = DecisionTreeClassifier(**self.kwargs)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает вероятности классов для каждого объекта на основе \n",
    "        предсказаний всех деревьев в лесу. Возвращает средние вероятности по всем деревьям.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания вероятностей классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples, n_classes)\n",
    "            Вероятности для каждого класса и каждого объекта.\n",
    "        \"\"\"\n",
    "        proba_predictions = np.array([estimator.predict_proba(X) for estimator in self.estimators])\n",
    "        return proba_predictions.mean(axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для каждого объекта входной выборки \n",
    "        на основе голосования всех деревьев.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples,)\n",
    "            Вектор предсказанных меток классов.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce673c-8699-4040-9622-07d85fa64e37",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c50d29-b19f-43d8-8f62-e79479856c04",
   "metadata": {},
   "source": [
    "   * Напишите ансамбль типа `Stacking` для решения задачи классификации. \n",
    "   * Подберите оптимальный набор из **не менее трех** базовых моделей и мета-модель. Установите их в качестве значений по умолчанию. Проверьте свое решение с помощью теста.\n",
    "> Подобранная конфигурация ансамбля должна проходить все тесты задания.\n",
    "> Для факторного пространства мета-модели используйте метод `predict_proba` базовых моделей.\n",
    "> В качестве базовых моделей и мета-модели используйте модели из библиотеки `scikit-learn`.\n",
    "> Для инференса модели используйте базовые модели, обученные на **ВСЁМ** датасете.\n",
    "\n",
    "```python run.py unittest stacking```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7260383a-78fa-45cf-90cd-bb69412f30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingClassifier:\n",
    "    \"\"\"\n",
    "    Модель ансамбля методом стекинга (Stacking).\n",
    "\n",
    "    Атрибуты:\n",
    "    ----------\n",
    "    estimators : list\n",
    "        Список инициализированных базовых моделей.\n",
    "\n",
    "    final_estimator : объект модели\n",
    "        Метамодель, обучаемая на мета-признаках (предсказаниях базовых моделей).\n",
    "\n",
    "    folds : int\n",
    "        Количество фолдов для кросс-валидации при обучении базовых моделей.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimators=None, final_estimator=None, folds=5):\n",
    "        if estimators is None:\n",
    "            self.estimators = [\n",
    "                DecisionTreeClassifier(random_state=42),\n",
    "                GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "                # ('svm', SVC(probability=True, random_state=42)),\n",
    "                GaussianNB()\n",
    "            ]\n",
    "        else:\n",
    "            self.estimators = estimators\n",
    "\n",
    "        if final_estimator is None:\n",
    "            self.final_estimator = LogisticRegression(random_state=42)\n",
    "        else:\n",
    "            self.final_estimator = final_estimator\n",
    "\n",
    "        self.folds = folds\n",
    "        self.base_models = []\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает базовые модели на тренировочных фолдах и использует\n",
    "        их предсказания на валидационных фолдах для обучения метамодели.\n",
    "        Применяется кросс-валидация с заданным количеством фолдов.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Тренировочные данные (признаки).\n",
    "\n",
    "        y : np.array, shape (n_samples,)\n",
    "            Целевые метки классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : Stacking\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.folds, shuffle=True, random_state=42)\n",
    "        n_classes = len(np.unique(y))\n",
    "        self.n_classes = n_classes\n",
    "        meta_features = np.zeros((X.shape[0], len(self.estimators) * (n_classes - 1)))\n",
    "\n",
    "        for i, model in enumerate(self.estimators):\n",
    "            model_meta_features = np.zeros((X.shape[0], n_classes - 1))\n",
    "            for train_index, val_index in kf.split(X):\n",
    "                X_train, X_val = X[train_index], X[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                model_meta_features[val_index] = model.predict_proba(X_val)[:, :n_classes - 1]\n",
    "            print(model_meta_features.shape)\n",
    "            meta_features[:, i * (n_classes - 1):(i + 1) * (n_classes - 1)] = model_meta_features\n",
    "\n",
    "            # Обучение модели на всем датасете для инференса\n",
    "            model.fit(X, y)\n",
    "            self.base_models.append(model)\n",
    "            # print(name)\n",
    "        # print(meta_features)\n",
    "        self.final_estimator.fit(meta_features, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает вероятности классов с помощью базовых моделей,\n",
    "        передает их метамодели для получения финального предсказания.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания вероятностей классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples, n_classes)\n",
    "            Предсказанные вероятности для каждого класса.\n",
    "        \"\"\"\n",
    "        meta_features = np.column_stack([model.predict_proba(X)[:, :self.n_classes - 1] for model in self.base_models])\n",
    "        return self.final_estimator.predict_proba(meta_features)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов на основе предсказаний базовых моделей\n",
    "        и метамодели.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples,)\n",
    "            Вектор предсказанных меток классов.\n",
    "        \"\"\"\n",
    "        return self.final_estimator.predict(np.column_stack([model.predict_proba(X)[:, :self.n_classes - 1] for model in self.base_models]))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Вычисляет softmax функцию для входного массива x.\n",
    "    \n",
    "    Softmax функция преобразует входные значения в вероятности, распределяя\n",
    "    их таким образом, что их сумма равна 1. Это полезно в задачах классификации,\n",
    "    где требуется получить вероятности принадлежности к каждому классу.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Входной массив значений размером (n_samples, n_classes), для которых необходимо вычислить softmax.\n",
    "    \n",
    "    Возвращает:\n",
    "    ----------\n",
    "    numpy.ndarray\n",
    "        Массив значений softmax, где каждый элемент является вероятностью, и сумма всех элементов равна 1.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def one_hot_encode(y, n_classes=None):\n",
    "    \"\"\"\n",
    "    Выполняет one-hot кодирование для заданного списка меток.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    y : numpy.ndarray или list\n",
    "        Вектор или список меток классов, которые необходимо закодировать.\n",
    "        Значения меток должны быть целыми числами от 0 до n_classes-1.\n",
    "\n",
    "    n_classes : int или None, по умолчанию None\n",
    "        Количество классов (размерность выходного пространства).\n",
    "        Если None, то количество классов определяется автоматически как максимум значения в y плюс один.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    numpy.ndarray\n",
    "        Массив размером (n_samples, n_classes), где n_samples — количество образцов, а n_classes — количество классов.\n",
    "        Каждая строка представляет собой one-hot закодированное представление соответствующей метки из y.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(y) + 1\n",
    "    one_hot = np.zeros(len(y), n_classes)\n",
    "    one_hot[np.arrange(len(y))] = 1\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b36838-20f2-4c9d-9fb6-6f33e74f2d26",
   "metadata": {},
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffeb2e-7822-45aa-9d4e-54a0087588e3",
   "metadata": {},
   "source": [
    "   * Напишите ансамбль типа `Blending` для решения задачи классификации. \n",
    "   * Подберите оптимальный набор из **не менее трех** базовых моделей и мета-модель. Установите их в качестве значений по умолчанию. Проверьте свое решение с помощью теста.\n",
    "   \n",
    "> Подобранная конфигурация ансамбля должна проходить все тесты задания.\n",
    "> Для факторного пространства мета-модели используйте метод `predict_proba` базовых моделей.\n",
    "> В качестве базовых моделей и мета-модели используйте модели из библиотеки `scikit-learn`.\n",
    "\n",
    "```python run.py unittest blending```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3976672f-6b48-48dd-bdbe-48d4438da107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendingClassifier:\n",
    "    \"\"\"\n",
    "    Модель ансамбля методом блендинга (Blending).\n",
    "\n",
    "    Атрибуты:\n",
    "    ----------\n",
    "    estimators : list\n",
    "        Список инициализированных базовых моделей.\n",
    "\n",
    "    final_estimator : объект модели\n",
    "        Метамодель, обучаемая на предсказаниях базовых моделей.\n",
    "\n",
    "    test_size : float\n",
    "        Доля данных, используемая для обучения метамодели (блендинга).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimators=None, final_estimator=None, test_size=0.2):\n",
    "        if estimators is None:\n",
    "            self.estimators = [\n",
    "                LogisticRegression(random_state=42, max_iter=200, solver=\"newton-cg\"),\n",
    "                KNeighborsClassifier(),\n",
    "                DecisionTreeClassifier(random_state=42)\n",
    "            ]\n",
    "        else:\n",
    "            self.estimators = estimators\n",
    "\n",
    "        if final_estimator is None:\n",
    "            self.final_estimator = LogisticRegression(random_state=42, max_iter=200, solver='lbfgs')\n",
    "        else:\n",
    "            self.final_estimator = final_estimator\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Разделяет входную выборку на тренировочную и валидационную части.\n",
    "        Базовые модели обучаются на тренировочной части, а метамодель — на предсказаниях\n",
    "        базовых моделей на валидационной части.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Тренировочные данные (признаки).\n",
    "\n",
    "        y : np.array, shape (n_samples,)\n",
    "            Целевые метки классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : Blending\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "        self.kClasses = len(np.unique(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size, random_state=42)\n",
    "        for estimator in self.estimators:\n",
    "            estimator.fit(X_train, y_train)\n",
    "\n",
    "        X_meta_train = np.hstack([\n",
    "            estimator.predict_proba(X_val)[:, :self.kClasses - 1] for estimator in self.estimators\n",
    "        ])\n",
    "        self.final_estimator.fit(X_meta_train, y_val)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает вероятности классов с использованием базовых моделей,\n",
    "        передает их метамодели для получения финального предсказания.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания вероятностей классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples, n_classes)\n",
    "            Предсказанные вероятности для каждого класса.\n",
    "        \"\"\"\n",
    "        X_meta = np.column_stack([\n",
    "            estimator.predict_proba(X)[:, :self.kClasses - 1] for estimator in self.estimators\n",
    "        ])\n",
    "        return self.final_estimator.predict_proba(X_meta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов на основе предсказаний базовых моделей\n",
    "        и метамодели.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples,)\n",
    "            Вектор предсказанных меток классов.\n",
    "        \"\"\"\n",
    "        X_meta = np.column_stack([\n",
    "            estimator.predict_proba(X)[:, :self.kClasses - 1] for estimator in self.estimators\n",
    "        ])\n",
    "        return self.final_estimator.predict(X_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0583e5",
   "metadata": {},
   "source": [
    "## Задание №4 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c531062",
   "metadata": {},
   "source": [
    "   * Напишите ансамбль типа `Boosting` для решения задачи классификации. \n",
    "   * Подберите оптимальные гиперпараметры модели и установите их в качестве значений по умолчанию. Проверьте свое решение с помощью теста.\n",
    "   \n",
    "> Подобранная конфигурация ансамбля должна проходить все тесты задания.\n",
    "> Каждое дерево предсказывает логиты. Для подсчета вероятностей используйте softmax. \n",
    "> Результирующие логиты инициализируйте нулями.\n",
    "> Для оптимизации используйте функционал cross-entropy.\n",
    "\n",
    "```python run.py unittest boosting```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce6abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    \"\"\"\n",
    "    Модель Бустинга (BoostingClassifier).\n",
    "\n",
    "    Атрибуты:\n",
    "    ----------\n",
    "    n_estimators : int\n",
    "        Количество деревьев в ансамбле.\n",
    "    \n",
    "    bootstrap : bool\n",
    "        Используется ли бутстрап при построении деревьев. \n",
    "        Если False, то для каждого дерева используется весь набор данных.\n",
    "    \n",
    "    estimators : list\n",
    "        Список деревьев с заданными параметрами (**kwargs).\n",
    "    \n",
    "    kwargs : dict\n",
    "        Параметры для каждого дерева, такие как min_samples_split, max_depth и др.\n",
    "        Передаются в DecisionTreeRegressor.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, bootstrap=True, lr=0.1, **kwargs):\n",
    "        self.bootstrap = bootstrap\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.kwargs = kwargs\n",
    "        self.estimators = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает ансамбль на тренировочной выборке (X, y). \n",
    "        Если bootstrap=False, используется весь набор данных для каждого дерева,\n",
    "        если bootstrap=True, используются бутстрап-выборки.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Тренировочные данные (признаки).\n",
    "        \n",
    "        y : np.array, shape (n_samples,)\n",
    "            Целевые метки классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : BoostingClassifier\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "        n_samples, n_classes = X.shape[0], len(np.unique(y))\n",
    "\n",
    "        self.F = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute gradients\n",
    "            probs = softmax(self.F)\n",
    "            gradients = probs - np.eye(n_classes)[y]\n",
    "\n",
    "            trees = []\n",
    "            for k in range(n_classes):\n",
    "                if self.bootstrap:\n",
    "                    X_boot, grad_boot = resample(X, gradients[:, k])\n",
    "                else:\n",
    "                    X_boot, grad_boot = X, gradients[:, k]\n",
    "\n",
    "                tree = DecisionTreeRegressor(**self.kwargs)\n",
    "                tree.fit(X_boot, grad_boot)\n",
    "                trees.append(tree)\n",
    "\n",
    "            self.estimators.append(trees)\n",
    "\n",
    "            for k in range(n_classes):\n",
    "                self.F[:, k] -= self.lr * self.estimators[-1][k].predict(X)\n",
    "\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает вероятности классов для каждого объекта входной выборки.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания вероятностей классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples, n_classes)\n",
    "            Вероятности для каждого класса и каждого объекта.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.estimators[0])\n",
    "\n",
    "        F = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        for trees in self.estimators:\n",
    "            for k, tree in enumerate(trees):\n",
    "                F[:, k] -= self.lr * tree.predict(X)\n",
    "\n",
    "        return softmax(F)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для каждого объекта входной выборки.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples,)\n",
    "            Вектор предсказанных меток классов.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
