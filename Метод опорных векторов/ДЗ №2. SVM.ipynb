{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2. SVM.\n",
    "\n",
    "В этом домашнем задании вам требуется написать свою модель SVM для решения задачи многоклассовой классификации. Для решения задания допускается использование библиотек `numpy`, `pandas` и `scipy`. Решение сдается коммитом в этот репозиторий заполненного файла `solution.py` и изображений `images/*`. За задание максимально можно набрать **30 баллов**. Вес каждого пункта задания указан в самом задании.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "#### Линейный SVM (15 баллов)\n",
    "Реализуйте класс `BinaryEstimatorSVM` для обучения бинарного метода опорных векторов с помощью mini-batch градиентного спуска. Для этого реализуйте следующие функции и методы:\n",
    " * **(1 балл)** `predict` - метод для предсказания меток класса. Обратите внимание, что **свободный член прибавляется со знаком плюс**. Проверьте свое решение локально с помощью команды: `python run.py unittest binary_predict`.\n",
    " * **(2 балла)** `loss` - метод для подсчета функции ошибки. Проверьте свое решение локально с помощью команды: `python run.py unittest binary_loss`. Обратите внимание, что агрегация значения лосса по батчу производится усреднением.\n",
    " * **(2 балла)** `loss_grad` - метод для подсчета градиента функции ошибки для метода опорных векторов. Проверьте свое решение локально с помощью команды: `python run.py unittest loss_grad`.\n",
    " * **(1 балл)** `step` - метод шага обновления параметров модели **методом градиентного спуска**. Проверьте свое решение локально с помощью команды: `python run.py unittest binary_step`.\n",
    " * **(4 балла)** `fit` - метод для обучения весов модели методом mini-batch c помощью \n",
    "градиентного спуска. Проверьте свое решение локально с помощью команды: `python run.py unittest binlinear_svm`.\n",
    "\n",
    "Реализуйте класс `LinearPrimalSVM` для обучения многоклассового метода опорных векторов с помощью стратегии one vs rest на бинарных моделях класса `BinaryEstimatorSVM`. Для этого реализуйте следующие функции и методы:\n",
    " * **(1 балл)** `one_vs_rest` - функция преобразует целевые метки в матрицу, где метки целевого класса принимают значение 1, а остальные метки — значение -1. Проверьте свое решение локально с помощью команды: `python run.py unittest one_vs_rest`.\n",
    " * **(2 балла)** `predict` - метод для предсказания меток класса. Проверьте свое решение локально с помощью команды: `python run.py unittest linear_predict_svm`. \n",
    " * **(2 балла)** `fit` - метод для обучения многоклассовой модели с использованием `BinaryEstimatorSVM`. Проверьте свое решение локально с помощью команды: `python run.py unittest linear_svm`. В этом тесте производится обучение вашей модели на трёх различных датасетах. Проверка правильности вашей реализации производится автоматически через метрику accuracy, которая будет провалидирована вручную после дедлайна по визуализации разделяющих кривых. Разделяющие кривые будут автоматически сгенерированы и сохранены в папку [images/](images/) в процессе тестирования. Они также будут отображены в раделе [**Загрузка решения**](#linear). **САМОСТОЯТЕЛЬНО ГРАФИКИ СТРОИТЬ НЕ НУЖНО!**. \n",
    "      \n",
    "#### Нелинейный SVM (15 баллов)\n",
    "Реализовать **NonlinearDualSVM** - метод решения **много-классового нелинейного метода опорных векторов двойственной задаче  оптимизации с квадратичной регуляризацией с поддежкой любого нелинейного ядра**. Алгоритм решения SVM через двойственную задачу можно взять из ноутбука, который мы рассматривали в классе - вам нужно поддержать многоклассовое решение задачи и поддержку ядра для решений в нелинейном случае.\n",
    "\n",
    "##### Подсказки\n",
    "Получить получить ядерную матрицу Грамма можно так:\n",
    "\n",
    "```python\n",
    "# X shape is (N, dim)\n",
    "# y_hat shape is (N, 1)\n",
    "\n",
    "def kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "kernalized_X = np.apply_along_axis(lambda x1 : np.apply_along_axis(lambda x2: kernel(x1, x2), 1, X), 1, X)  \n",
    "gram_matrix_Xy = kernalized_X * np.matmul(y_hat, y_hat.T)\n",
    "```\n",
    "\n",
    "Отделить опорные вектора от неопорных можно следующим образом:\n",
    "```python\n",
    "(self.alpha > epsilon) & (self.alpha <= self.C + epsilon)\n",
    "```\n",
    "\n",
    "Для обучения много-классового алгоритма воспользоваться схемой **one-vs-one**.\n",
    "Реализуйте класс `SoftMarginSVM` для решения нелинейной двойственной задачи метода опорных векторов с помощью метода SLSQP из scipy. Для этого реализуйте следующие функции и методы:\n",
    " * **(1.5 баллов)** `kernel_linear`, `kernel_poly`, `kernel_rbf`  - функции ядер. Проверьте свое решение локально с помощью команды: `python run.py unittest kernels`.\n",
    " * **(1 балла)** `lagrange`, `lagrange_derive` - функции для подсчета функции Лагранжа для метода опорных векторов и её производной. Проверьте свое решение локально с помощью команды: `python run.py unittest lagrange`.\n",
    " * **(2.5 балла)** `predict` - метод для предсказания меток класса. Проверьте свое решение локально с помощью команды: `python run.py unittest binary_nonlinear_predict`.\n",
    " * **(5 балла)** `fit` - метод для обучения бинарной модели метода опорных векторов с помощью SLSQP из scipy. Проверьте свое решение локально с помощью команды: `python run.py unittest binnonlinear_svm`.\n",
    "   \n",
    "Реализуйте класс `NonLinearDualSVM` для обучения многоклассового метода опорных векторов с помощью стратегии one vs one на бинарных моделях класса `SoftMarginSVM`. Для этого реализуйте следующие функции и методы:\n",
    " * **(1 баллов)** `one_vs_one` - фунция, которая преобразует целевые метки в матрицу, где метки первого класса принимают значение 1, а метки второго — значение -1. Проверьте свое решение локально с помощью команды: `python run.py unittest one_vs_one`.\n",
    " * **(2 баллов)** `predict` - метод для предсказания меток класса. Проверьте свое решение локально с помощью команды: `python run.py unittest nonlinear_predict_svm`. \n",
    " * **(2 балла)** `fit` - метод для обучения многоклассовой модели с использованием `SoftMarginSVM`. Проверьте свое решение локально с помощью команды: `python run.py unittest nonlinear_svm`. Аналогично тесту линейного классификатора будет сгенерирована [разделяющая поверхность](#nonlinear).\n",
    "\n",
    "#### Разделяющие прямые построенного линейного классификатора <a name=\"linear\"></a>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/LinearSVM1.jpg\" alt=\"LinearSVM1.jpg\" width=\"233\"/> <img src=\"./images/LinearSVM2.jpg\" alt=\"LinearSVM2.jpg\" width=\"233\"/> <img src=\"./images/LinearSVM3.jpg\" alt=\"LinearSVM3.jpg\" width=\"233\"/>\n",
    "</p>\n",
    "\n",
    "#### Разделяющие кривые построенного нелинейного классификатора <a name=\"nonlinear\"></a>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/NonLinearSVM.jpg\" alt=\"NonLinearSVM.jpg\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "class BinaryEstimatorSVM:\n",
    "    \"\"\"\n",
    "    Класс для построения модели бинарной классификации методом опорных \n",
    "    векторов путем решения прямой задачи оптимизации.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    lr : float, default=0.01\n",
    "        Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "\n",
    "    n_epochs : int, default=100\n",
    "        Количество эпох для обучения модели.\n",
    "\n",
    "    batch_size : int, default=16\n",
    "        Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "    \n",
    "    Атрибуты:\n",
    "    ---------\n",
    "    lr : float, default=0.01\n",
    "        Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "\n",
    "    n_epochs : int, default=100\n",
    "        Количество эпох для обучения модели.\n",
    "\n",
    "    batch_size : int, default=16\n",
    "        Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "    fit_intercept : bool, по умолчанию True\n",
    "        Включать ли свободный член (сдвиг) в модель.\n",
    "\n",
    "    drop_last : bool, по умолчанию True\n",
    "        Удалять ли последний неполный батч из обучения.\n",
    "\n",
    "    coef_ : numpy.ndarray или None\n",
    "        Коэффициенты (веса) модели размером (n_features, 1), которые обучаются на данных. Инициализируются как None до вызова метода `fit`.\n",
    "\n",
    "    intercept_ : numpy.ndarray или None\n",
    "        Свободный член (сдвиг) модели размером (1). Инициализируется как None до вызова метода `fit`.\n",
    "\n",
    "    n_classes_ : int\n",
    "        Количество классов.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, C=1.0, n_epochs=100, batch_size=16, fit_intercept=True, drop_last=True):\n",
    "        \"\"\"\n",
    "        Инициализация объекта класса LinearPrimalSVM с заданными гиперпараметрами.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        lr : float, default=0.01\n",
    "            Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "        C : float, default=1.0\n",
    "            Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "\n",
    "        n_epochs : int, default=100\n",
    "            Количество эпох для обучения модели.\n",
    "\n",
    "        batch_size : int, default=16\n",
    "            Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "        fit_intercept : bool, по умолчанию True\n",
    "            Включать ли свободный член (сдвиг) в модель.\n",
    "\n",
    "        drop_last : bool, по умолчанию True\n",
    "            Удалять ли последний неполный батч из обучения.\n",
    "        \"\"\"\n",
    "\n",
    "        self.lr = lr\n",
    "        self.C = C\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.drop_last = drop_last\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.n_classes_ = None\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает расстояние до разделяющей классы гиперплоскости для входных данных на основе обученной модели.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        numpy.ndarray\n",
    "            Вектор предсказанных расстояний, ориентированных по нормали к разделяющей гиперплоскости.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"invalid coef_\")\n",
    "\n",
    "        decision = np.dot(X, self.coef_)\n",
    "\n",
    "        if self.fit_intercept and self.intercept_ is not None:\n",
    "            decision += self.intercept_\n",
    "\n",
    "        return decision\n",
    "\n",
    "\n",
    "    def loss(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Вычисляет функцию потерь для бинарной классификации на основе HingeLoss\n",
    "        с учетом L2 регуляризации\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features), где n_samples — количество образцов,\n",
    "            а n_features — количество признаков.\n",
    "\n",
    "        y_true : numpy.ndarray\n",
    "            Вектор истинных меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        float\n",
    "            Значение функции потерь.\n",
    "        \"\"\"\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"invalid coef_\")\n",
    "\n",
    "        y_true = y_true.reshape(-1)\n",
    "\n",
    "        predictions = self.predict(X)\n",
    "\n",
    "        margins = y_true * predictions\n",
    "        hinge_loss = np.maximum(0, 1 - margins).mean()\n",
    "\n",
    "        l2_reg = 0.5 * np.dot(self.coef_.T, self.coef_).item()\n",
    "\n",
    "        return hinge_loss + self.C * l2_reg\n",
    "\n",
    "\n",
    "    def loss_grad(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Вычисляет градиент функции потерь по отношению к весам модели.\n",
    "\n",
    "        В случае использования регуляризации, градиент включает соответствующие компоненты для\n",
    "        штрафа за большие значения весов.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "\n",
    "        X : numpy.ndarray\n",
    "            Входной массив признаков размером (n_samples, n_features), где n_samples — количество образцов,\n",
    "            а n_features — количество признаков.\n",
    "\n",
    "        y_true : numpy.ndarray\n",
    "            Вектор истинных меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        grad : numpy.ndarray\n",
    "            Градиент функции потерь по отношению к весам модели.\n",
    "\n",
    "        grad_intercept : numpy.ndarray\n",
    "            Градиент функции потерь по отношению к свободному члену.\n",
    "        \"\"\"\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"invalid coef_\")\n",
    "\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "        margins = y_true * self.predict(X)\n",
    "\n",
    "        indicator = (margins < 1).astype(float)\n",
    "\n",
    "        grad = -np.dot(X.T, y_true * indicator) + self.C * self.coef_\n",
    "\n",
    "        grad_intercept = 0\n",
    "        if self.fit_intercept:\n",
    "            grad_intercept = -np.sum(y_true * indicator)\n",
    "\n",
    "        return grad, grad_intercept\n",
    "\n",
    "    def step(self, grad, grad_intercept):\n",
    "        \"\"\"\n",
    "        Выполняет один шаг обновления весов модели с использованием вычисленного градиента.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        grad : numpy.ndarray\n",
    "            Градиент функции потерь по отношению к весам модели (размером как coef_).\n",
    "        \n",
    "        grad_intercept : numpy.ndarray или None\n",
    "            Градиент функции потерь по отношению к свободному члену (размером как intercept_).\n",
    "            Если fit_intercept=False, этот параметр будет равен None.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.coef_ -= self.lr * grad\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ -= self.lr * grad_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель SVM с использованием mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Тренировочные данные.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples,)\n",
    "            Целевые метки классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : LinearPrimalSVM\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        if self.coef_ is None:\n",
    "            self.coef_ = np.zeros((X.shape[1], 1))\n",
    "        if self.intercept_ is None and self.fit_intercept:\n",
    "            self.intercept_ = 0.0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                if self.drop_last and end > n_samples:\n",
    "                    break\n",
    "                batch_indices = indices[start:end]\n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "\n",
    "                grad, grad_intercept = self.loss_grad(X_batch, y_batch)\n",
    "\n",
    "                self.step(grad, grad_intercept)\n",
    "        return self\n",
    "\n",
    "def one_vs_rest(y, n_classes=None):\n",
    "    \"\"\"\n",
    "    Преобразует целевые метки в матрицу, где метки целевого класса\n",
    "    принимают значение 1, а остальные метки — значение -1.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    y : numpy.ndarray или list\n",
    "        Вектор или список меток классов, которые необходимо закодировать.\n",
    "        Значения меток должны быть целыми числами от 0 до n_classes-1.\n",
    "\n",
    "    n_classes : int или None, по умолчанию None\n",
    "        Количество классов (размерность выходного пространства).\n",
    "        Если None, то количество классов определяется автоматически как максимум значения в y плюс один.\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    numpy.ndarray\n",
    "        Двумерная матрица размером (n_samples, n_classes), где для каждого образца целевой\n",
    "        класс представлен значением 1, а все остальные классы имеют значение -1.\n",
    "\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(y) + 1\n",
    "\n",
    "    one_vs_rest_matrix = -1 * np.ones((y.shape[0], n_classes))\n",
    "\n",
    "    one_vs_rest_matrix[np.arange(y.shape[0]), y] = 1\n",
    "\n",
    "    return one_vs_rest_matrix\n",
    "\n",
    "\n",
    "class LinearPrimalSVM:\n",
    "    \"\"\"\n",
    "    Класс для построения модели многоклассовой классификации методом опорных \n",
    "    векторов путем решения прямой задачи оптимизации.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    lr : float, default=0.01\n",
    "        Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Параметр регуляризации, контролирующий баланс между максимизацией зазора \n",
    "        и минимизацией ошибок классификации.\n",
    "\n",
    "    n_epochs : int, default=100\n",
    "        Количество эпох для обучения модели.\n",
    "\n",
    "    batch_size : int, default=16\n",
    "        Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "    \n",
    "    Атрибуты:\n",
    "    ---------\n",
    "    lr : float, default=0.01\n",
    "        Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "\n",
    "    n_epochs : int, default=100\n",
    "        Количество эпох для обучения модели.\n",
    "\n",
    "    batch_size : int, default=16\n",
    "        Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "    fit_intercept : bool, по умолчанию True\n",
    "        Включать ли свободный член (сдвиг) в модель.\n",
    "\n",
    "    drop_last : bool, по умолчанию True\n",
    "        Удалять ли последний неполный батч из обучения.\n",
    "\n",
    "    self.n_classes_ : int\n",
    "        Количество классов, определяемое на основе уникальных меток в обучающем наборе данных.\n",
    "        Этот параметр устанавливается после вызова метода `fit` и используется для определения \n",
    "        размерности выходного пространства модели. Он равен максимальному значению метки в данных плюс один.\n",
    "\n",
    "    list_of_models : list\n",
    "        Список, содержащий бинарные модели.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, C=1.0, n_epochs=100, batch_size=16, fit_intercept=True, drop_last=True):\n",
    "        \"\"\"\n",
    "        Инициализация объекта класса LinearPrimalSVM с заданными гиперпараметрами.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        lr : float, default=0.01\n",
    "            Скорость обучения (learning rate) для обновления коэффициентов модели.\n",
    "\n",
    "        C : float, default=1.0\n",
    "            Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "\n",
    "        n_epochs : int, default=100\n",
    "            Количество эпох для обучения модели.\n",
    "\n",
    "        batch_size : int, default=16\n",
    "            Размер батча для mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "        fit_intercept : bool, по умолчанию True\n",
    "            Включать ли свободный член (сдвиг) в модель.\n",
    "\n",
    "        drop_last : bool, по умолчанию True\n",
    "            Удалять ли последний неполный батч из обучения.\n",
    "        \"\"\"\n",
    "\n",
    "        self.lr = lr\n",
    "        self.C = C\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.drop_last = drop_last\n",
    "        self.n_classes_ = None\n",
    "        self.list_of_models = []\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для входных данных на основе обученной модели.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        numpy.ndarray\n",
    "            Вектор предсказанных меток классов (значения от 0 до n_classes-1).\n",
    "        \"\"\"\n",
    "        if not self.list_of_models:\n",
    "            raise ValueError(\"Модель не была обучена. Сначала используйте метод `fit`.\")\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        scores = np.zeros((n_samples, self.n_classes_))\n",
    "\n",
    "        for i, model in enumerate(self.list_of_models):\n",
    "            scores[:, i] = model.predict(X).flatten()\n",
    "\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель SVM с использованием mini-batch градиентного спуска (mini-batch GD).\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Тренировочные данные.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples,)\n",
    "            Целевые метки классов.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : LinearPrimalSVM\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "        y_encoded = one_vs_rest(y, n_classes=self.n_classes_)\n",
    "        self.n_classes_ = y_encoded.shape[1]\n",
    "        self.list_of_models = []\n",
    "\n",
    "        for i in range(self.n_classes_):\n",
    "            model = BinaryEstimatorSVM(lr=self.lr, C=self.C, n_epochs=self.n_epochs, batch_size=self.batch_size)\n",
    "\n",
    "            model.fit(X, y_encoded[:, i])\n",
    "\n",
    "            self.list_of_models.append(model)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "def kernel_linear(x1, x2):\n",
    "    \"\"\"\n",
    "    Линейное ядро для SVM.\n",
    "\n",
    "    Вычисляет скалярное произведение двух векторов, что соответствует линейной \n",
    "    границе разделения в пространстве признаков.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    x1 : np.array, shape (n_features,)\n",
    "        Первый вектор признаков.\n",
    "    \n",
    "    x2 : np.array, shape (n_features,)\n",
    "        Второй вектор признаков.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    float\n",
    "        Скалярное произведение векторов x1 и x2.\n",
    "    \"\"\"\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "\n",
    "def kernel_poly(x1, x2, d=2):\n",
    "    \"\"\"\n",
    "    Полиномиальное ядро для SVM.\n",
    "\n",
    "    Вычисляет полиномиальное скалярное произведение двух векторов, \n",
    "    что позволяет моделировать нелинейные границы разделения.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    x1 : np.array, shape (n_features,)\n",
    "        Первый вектор признаков.\n",
    "    \n",
    "    x2 : np.array, shape (n_features,)\n",
    "        Второй вектор признаков.\n",
    "    \n",
    "    d : int, default=2\n",
    "        Степень полинома.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    float\n",
    "        Полиномиальное скалярное произведение векторов x1 и x2.\n",
    "    \"\"\"\n",
    "    return (np.dot(x1, x2) + 1) ** d\n",
    "\n",
    "def kernel_rbf(x1, x2, l=1.0):\n",
    "    \"\"\"\n",
    "    Радиально-базисное (гауссовское) ядро для SVM.\n",
    "\n",
    "    Вычисляет расстояние между двумя векторами с использованием радиально-базисной функции (RBF),\n",
    "    которая позволяет моделировать сложные нелинейные зависимости.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    x1 : np.array, shape (n_features,)\n",
    "        Первый вектор признаков.\n",
    "    \n",
    "    x2 : np.array, shape (n_features,)\n",
    "        Второй вектор признаков.\n",
    "    \n",
    "    l : float, default=1.0\n",
    "        Параметр ширины гауссовской функции (коэффициент сглаживания).\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    float\n",
    "        Значение RBF-ядра между векторами x1 и x2.\n",
    "    \"\"\"\n",
    "    distance = np.linalg.norm(x1 - x2)\n",
    "    return np.exp(- (distance ** 2) / (2 * (l ** 2)))\n",
    "\n",
    "def lagrange(gramm_matrix, alpha):\n",
    "    \"\"\"\n",
    "    Двойственная функция Лагранжа для SVM.\n",
    "\n",
    "    Вычисляет двойственную функцию для оптимизации SVM с использованием\n",
    "    заранее рассчитанной матрицы Грамма.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    gramm_matrix : np.array, shape (n_samples, n_samples)\n",
    "        Матрица Грамма (значения ядер между всеми парами обучающих объектов).\n",
    "    \n",
    "    alpha : np.array, shape (n_samples,)\n",
    "        Двойственные переменные (лямбда), используемые для оптимизации.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    float\n",
    "        Значение двойственной функции Лагранжа.\n",
    "    \"\"\"\n",
    "    return np.sum(alpha) - 0.5 * np.dot(alpha, np.dot(gramm_matrix, alpha))\n",
    "\n",
    "def lagrange_derive(gramm_matrix, alpha):\n",
    "    \"\"\"\n",
    "    Производная двойственной функции Лагранжа по alpha.\n",
    "\n",
    "    Вычисляет градиент (производную) двойственной функции Лагранжа,\n",
    "    что необходимо для решения задачи оптимизации.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    gramm_matrix : np.array, shape (n_samples, n_samples)\n",
    "        Матрица Грама (значения ядер между всеми парами обучающих объектов).\n",
    "    \n",
    "    alpha : np.array, shape (n_samples,)\n",
    "        Двойственные переменные (лямбда), используемые для оптимизации.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    np.array, shape (n_samples,)\n",
    "        Градиент двойственной функции по alpha.\n",
    "    \"\"\"\n",
    "    gradient = np.ones_like(alpha) - np.dot(gramm_matrix, alpha)\n",
    "    return gradient\n",
    "\n",
    "def one_vs_one(X, y, n_classes=None):\n",
    "    \"\"\"\n",
    "    Преобразует целевые метки в матрицу, где метки первого класса\n",
    "    принимают значение 1, а метки второго — значение -1.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    y : numpy.ndarray\n",
    "        Вектор или список меток классов, которые необходимо закодировать.\n",
    "        Значения меток должны быть целыми числами от 0 до n_classes-1.\n",
    "\n",
    "    n_classes : int или None, по умолчанию None\n",
    "        Количество классов (размерность выходного пространства).\n",
    "        Если None, то количество классов определяется автоматически как максимум значения в y плюс один.\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    list of tuples\n",
    "        (X_cut, y_cut (Бинарный таргет 1 или -1), соответствующий '1' класс, соответствующий '-1' класс)\n",
    "        \n",
    "    \"\"\"\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(y) + 1\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i + 1, n_classes):\n",
    "            mask = (y == i) | (y == j)\n",
    "            X_cut = X[mask]\n",
    "            y_cut = y[mask]\n",
    "\n",
    "            y_cut = np.where(y_cut == i, 1, -1)\n",
    "\n",
    "            results.append((X_cut, y_cut, i, j))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class SoftMarginSVM:\n",
    "    \"\"\"\n",
    "    Реализация SVM с мягким зазором (Soft Margin SVM) с возможностью использовать произвольные ядра.\n",
    "    \n",
    "    Атрибуты:\n",
    "    ----------\n",
    "    C : float, default=1.0\n",
    "        Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "    \n",
    "    alpha : np.array, shape (n_samples,)\n",
    "        Двойственные переменные (лямбда) для решения задачи оптимизации.\n",
    "\n",
    "    supportVectors : np.array, shape (n_support_vectors, n_features)\n",
    "        Опорные вектора — обучающие объекты, которые оказывают влияние на разделяющую гиперплоскость.\n",
    "\n",
    "    supportLabels : np.array, shape (n_support_vectors,)\n",
    "        Метки классов для опорных векторов.\n",
    "\n",
    "    supportalpha : np.array, shape (n_support_vectors,)\n",
    "        Значения альфа (лямбда) для опорных векторов.\n",
    "\n",
    "    kernel : function\n",
    "        Ядро для вычисления скалярных произведений в пространстве признаков.\n",
    "\n",
    "    classes_names : list or array-like, shape (2,)\n",
    "        Имена классов. Используются для преобразования предсказанных значений {-1, 1} в имена классов.\n",
    "    \n",
    "    b: float \n",
    "        Смещение.\n",
    "\n",
    "    Методы:\n",
    "    -------\n",
    "    fit(X, y):\n",
    "        Обучает модель.\n",
    "\n",
    "    predict(X):\n",
    "        Предсказывает метки классов для входных данных.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C, kernel_func, classes_names=None):\n",
    "        \"\"\"\n",
    "        Инициализирует модель Soft Margin SVM.\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        C : float, default=1.0\n",
    "            Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "        \n",
    "        kernel_func : function\n",
    "            Функция ядра, определяющая метод вычисления скалярных произведений в новом пространстве признаков.\n",
    "        \n",
    "        classes_names : list\n",
    "            Список имен классов. Ожидается, что в обучающих данных метки классов {-1, 1}.\n",
    "        \"\"\"\n",
    "        self.C = C                                 \n",
    "        self.alpha = None\n",
    "        self.supportVectors = None\n",
    "        self.supportLabels = None\n",
    "        self.supportalpha = None\n",
    "        self.kernel = kernel_func\n",
    "        self.classes_names = classes_names\n",
    "        self.b = None\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель с использованием оптимизации двойственной задачи для SVM.\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Обучающие данные (матрица признаков).\n",
    "        \n",
    "        y : np.array, shape (n_samples,)\n",
    "            Вектор меток классов, должен содержать значения {-1, 1}.\n",
    "        \n",
    "        Возвращает:\n",
    "        ----------\n",
    "        self : SoftMarginSVM\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "        N = len(y)\n",
    "        Xy = X * y.reshape(-1, 1)\n",
    "        GramXy = np.matmul(Xy, Xy.T)\n",
    "\n",
    "        first_constraint = {'type': 'eq', 'fun': lambda a: np.dot(a, y), 'jac': lambda a: y}\n",
    "        A = np.vstack((-np.eye(N), np.eye(N)))\n",
    "        b = np.hstack((np.zeros(N), self.C * np.ones(N)))\n",
    "\n",
    "        second_constraint = {'type': 'ineq', 'fun': lambda a: b - np.dot(A, a), 'jac': lambda a: -A}\n",
    "        constraints = (first_constraint, second_constraint)\n",
    "\n",
    "        opt_res = optimize.minimize(fun=lambda a: -lagrange(GramXy, a),\n",
    "                                    x0=np.ones(N),\n",
    "                                    method='SLSQP',\n",
    "                                    jac=lambda a: -lagrange_derive(GramXy, a),\n",
    "                                    constraints=constraints)\n",
    "\n",
    "        self.alpha = opt_res.x\n",
    "        self.w = np.sum((self.alpha.reshape(-1, 1) * Xy), axis=0)\n",
    "\n",
    "        valid_indices = (self.alpha >  1e-6) & (self.alpha <= self.C)\n",
    "        self.supportVectors = X[valid_indices]\n",
    "        supportLabels = y[valid_indices]\n",
    "\n",
    "        if len(self.supportVectors) > 0:\n",
    "            b_values = []\n",
    "            for x_s, y_s in zip(self.supportVectors, supportLabels):\n",
    "                b_values.append(y_s - np.dot(self.w, x_s))\n",
    "            self.b = np.mean(b_values)\n",
    "        else:\n",
    "            print(\"No valid support vectors to compute bias.\")\n",
    "            self.b = 0\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для входных данных.\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Массив объектов для предсказания.\n",
    "        \n",
    "        Возвращает:\n",
    "        ----------\n",
    "        np.array, shape (n_samples,)\n",
    "            Вектор предсказанных меток классов, где метки соответствуют значениям из `classes_names`.\n",
    "        \"\"\"\n",
    "        assert (self.w is not None)\n",
    "        assert (self.w.shape[0] == X.shape[1])\n",
    "        # по формуле 2(w^Tx + b) - 1 получаем значение объекта +1 или -1\n",
    "        return 2 * (np.matmul(X, self.w) + self.b > 0) - 1\n",
    "\n",
    "class NonLinearDualSVM:\n",
    "    \"\"\"\n",
    "    NonLinearDualSVM реализует SVM one-vs-one с использованием двойственной задачи. \n",
    "    Поддерживает использование различных ядерных функций для задач классификации.\n",
    "\n",
    "    Атрибуты:\n",
    "    ---------\n",
    "    estimators : list или None\n",
    "        Список бинарный SVM моделе one-vs-one\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "\n",
    "    kernel : str, default='rbf'\n",
    "        Ядерная функция, используемая в модели (возможные значения: 'poly', 'rbf', 'linear').\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=1.0, kernel='rbf', kernel_parameter=1.0):\n",
    "        \"\"\"\n",
    "        Инициализирует модель NonLinearDualSVM с указанной ядерной функцией.\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        C : float, default=1.0\n",
    "            Коэффициент, контролирующий баланс между минимизацией ошибок классификации и максимизацией зазора.\n",
    "\n",
    "        kernel : str, default='rbf'\n",
    "            Ядерная функция, используемая в модели (возможные значения: 'poly', 'rbf', 'linear').\n",
    "\n",
    "        kernel_parameter : float, default=1.0\n",
    "            Гиперпарметр ядра\n",
    "      \n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "\n",
    "        if kernel == 'poly':\n",
    "          self.kernel = lambda x, y: kernel_poly(x, y, d=kernel_parameter)\n",
    "        elif kernel == 'rbf':\n",
    "          self.kernel = lambda x, y: kernel_rbf(x, y, l=kernel_parameter)\n",
    "        else:\n",
    "          self.kernel = kernel_linear\n",
    "\n",
    "        self.kernel.__name__='kernel'\n",
    "\n",
    "        self.estimators = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для входных данных X.\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Входные данные для предсказания меток классов.\n",
    "\n",
    "        Возвращает:\n",
    "        -------\n",
    "        numpy.ndarray, shape (n_samples,)\n",
    "            Предсказанные метки классов для каждого образца.\n",
    "        \"\"\"\n",
    "\n",
    "        arr = X.shape[0]\n",
    "        array = []\n",
    "\n",
    "        i = 0\n",
    "        while i < arr:\n",
    "            accuracy = {}\n",
    "            for estimator in self.estimators:\n",
    "                predict = tuple(estimator.predict(X[i].reshape(1, -1))[0])\n",
    "                if predict in accuracy:\n",
    "                    accuracy[predict] = accuracy[predict] + 1\n",
    "                else:\n",
    "                    accuracy[predict] = 1\n",
    "\n",
    "            array.append(max(accuracy.items(), key=lambda x: x[1])[0])\n",
    "            i += 1\n",
    "\n",
    "        result = np.array(array)\n",
    "        return result.flatten()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель SVM на тренировочных данных (X, y) с использованием двойственной задачи.\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Тренировочные данные.\n",
    "\n",
    "        y : numpy.ndarray, shape (n_samples,)\n",
    "            Целевые метки классов.\n",
    "\n",
    "        Возвращает:\n",
    "        -------\n",
    "        self : NonLinearDualSVM\n",
    "            Обученная модель.\n",
    "        \"\"\"\n",
    "        \n",
    "        classes = np.max(y) + 1\n",
    "        one = one_vs_one(X, y, classes)\n",
    "\n",
    "        for X_subset, y_subset, class1, class2 in one:\n",
    "            svm_model = SoftMarginSVM(self.C, self.kernel, [class1, class2])\n",
    "            svm_model.fit(X_subset, y_subset)\n",
    "\n",
    "            self.estimators.append(svm_model)\n",
    "\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
